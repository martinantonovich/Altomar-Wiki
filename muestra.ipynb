{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias y lenguajes\n",
    "import spacy \n",
    "from spacy.tokens import Token, Span\n",
    "# nlp = spacy.blank(\"es\") # crea un pipeline vacio para procesar el espanol.\n",
    "nlp = spacy.load(\"es_core_news_md\") # Descarga el paquete del lenguaje espanol preentrenado para poder trabajar con las capacidades centrales.\n",
    "\n",
    "from spacy.matcher import Matcher # Importo el Matcher.\n",
    "matcher = Matcher(nlp.vocab) # Inicializo el matcher.\n",
    "\n",
    "from spacy.tokens import Doc, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arroz\n",
      "con\n",
      "leche\n",
      "comer\n",
      ",\n",
      "me\n",
      "quiero\n",
      "casar\n",
      "Con\n",
      "uno\n",
      "señora\n",
      "5\n",
      "de\n",
      "San\n",
      "Nicolás\n",
      "Que\n",
      "sepa\n",
      "cocer\n",
      ",\n",
      "que\n",
      "sepa\n",
      "bordar\n",
      "Que\n",
      "sepa\n",
      "el\n",
      "abrir\n",
      "la\n",
      "puerta\n",
      "para\n",
      "ir\n",
      "a\n",
      "jugar\n",
      "Yo\n",
      "soy\n",
      "20\n",
      "la\n",
      "viudita\n",
      "del\n",
      "barrio\n",
      "del\n",
      "rey\n",
      "Me\n",
      "quiero\n",
      "casar\n",
      "y\n",
      "no\n",
      "sé\n",
      "con\n",
      "quién\n",
      "Con\n",
      "esta\n",
      "sí\n",
      ",\n",
      "con\n",
      "esta\n",
      "no\n",
      "Con\n",
      "esta\n",
      "40\n",
      "señorita\n",
      "me\n",
      "caso\n",
      "yo\n",
      ".\n",
      "Arroz\n",
      "con\n",
      "leche\n",
      ",\n",
      "me\n",
      "quiero\n",
      "casar\n",
      "Con\n",
      "una\n",
      "señorita\n",
      "de\n",
      "san\n",
      "nicolás\n",
      "Que\n",
      "sepa\n",
      " \n",
      "la\n",
      "cocer\n",
      ",\n",
      "que\n",
      "sepa\n",
      "bordar\n",
      "Que\n",
      "sepa\n",
      "abrir\n",
      "la\n",
      "puerta\n",
      "para\n",
      "ir\n",
      "a\n",
      "jugar\n",
      "Yo\n",
      "soy\n",
      "la\n",
      "viudita\n",
      "del\n",
      "barrio\n",
      "del\n",
      "rey\n",
      "Me\n",
      "quiero\n",
      "casar\n",
      "y\n",
      "no\n",
      "sé\n",
      "con\n",
      "quién\n",
      "Con\n",
      "esta\n",
      "sí\n",
      ",\n",
      "con\n",
      "esta\n",
      "no\n",
      "Con\n",
      "esta\n",
      "señorita\n",
      "me\n",
      "caso\n",
      "yo\n"
     ]
    }
   ],
   "source": [
    "# Crear el texto, en un objeto llamado \"doc\".\n",
    "doc = nlp(\"Arroz con leche comer, me quiero casar\"\n",
    "        \" Con uno señora 5 de San Nicolás\"\n",
    "        \" Que sepa cocer, que sepa bordar\"\n",
    "        \" Que sepa el abrir la puerta para ir a jugar\"\n",
    "        \" Yo soy 20 la viudita del barrio del rey\"\n",
    "        \" Me quiero casar y no sé con quién\"\n",
    "        \" Con esta sí, con esta no\"\n",
    "        \" Con esta 40 señorita me caso yo.\"\n",
    "        \" Arroz con leche, me quiero casar\"\n",
    "        \" Con una señorita de san nicolás\"\n",
    "        \" Que sepa  la cocer, que sepa bordar\"\n",
    "        \" Que sepa abrir la puerta para ir a jugar\"\n",
    "        \" Yo soy la viudita del barrio del rey\"\n",
    "        \" Me quiero casar y no sé con quién\"\n",
    "        \" Con esta sí, con esta no\"\n",
    "        \" Con esta señorita me caso yo\")\n",
    "\n",
    "# Iterar sobre los tokens en el doc. Spacy transforma cada palabra, signo, puntuacion del doc en un token.\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me\n",
      "---------------------\n",
      ", me quiero casar Con uno señora 5 de\n",
      "---------------------\n",
      "Índice:    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "---------------------\n",
      "is_alpha: [True, True, True, True, False, True, True, True, True, True]\n",
      "is_punct: [False, False, False, False, True, False, False, False, False, False]\n",
      "like_num: [False, False, False, False, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# El objeto doc tiene su propio indice para el acceso directo.\n",
    "token = doc[5]\n",
    "print(token.text) # la funcion .text devuelve el texto.\n",
    "print(\"---------------------\")\n",
    "# Un objeto SPAN es un slice del doc, compuesto por 1 o mas tokens. Es un view, no contiene los datos en si.\n",
    "span = doc[4:13]\n",
    "print(span)\n",
    "print(\"---------------------\")\n",
    "print(\"Índice:   \", [token.i for token in doc[0:10]])\n",
    "print(\"---------------------\")\n",
    "# Reconocimiento de numeros, puntuaciones y palabras alfabeticas (llamados tambien atributos lexicos, no dependen del contexto):\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc[0:10]])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc[0:10]])\n",
    "print(\"like_num:\", [token.like_num for token in doc[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token encontrado: 40\n"
     ]
    }
   ],
   "source": [
    "# Itera sobre los tokens en el doc\n",
    "for token in doc:\n",
    "    # Revisa si el token parece un número\n",
    "    if token.like_num:\n",
    "        # Obtén el próximo token en el documento\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Revisa si el texto del siguiente token es igual a '%'\n",
    "        if next_token.text == \"señorita\":\n",
    "            print(\"Token encontrado:\", (token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arroz PROPN\n",
      "con ADP\n",
      "leche NOUN\n",
      "comer VERB\n",
      ", PUNCT\n",
      "me PRON\n",
      "quiero VERB\n",
      "casar VERB\n",
      "Con ADP\n",
      "uno PRON\n"
     ]
    }
   ],
   "source": [
    "# Componentes del pipeline entrenados.\n",
    "\n",
    "# Parser (Etiquetas gramaticales)\n",
    "for token in doc[0:10]:\n",
    "    # Imprime en pantalla el texto y la etiqueta gramatical predicha\n",
    "    print(token.text, token.pos_) # En spaCy, los atributos que devuelven un string normalmente terminan con un guion bajo (_). \n",
    "                                  # Mientras que atributos sin un guion bajo devuelven un valor ID de tipo entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arroz nsubj comer\n",
      "con case leche\n",
      "leche obl comer\n",
      "comer advcl quiero\n",
      ", punct comer\n",
      "me iobj quiero\n",
      "quiero ROOT quiero\n",
      "casar xcomp quiero\n",
      "Con case señora\n",
      "uno det señora\n"
     ]
    }
   ],
   "source": [
    "# Tagger (Relaciones entre las palabras)\n",
    "for token in doc[0:10]:\n",
    "    print(token.text, token.dep_, token.head.text) # El atributo .dep_ devuelve la etiqueta de la dependencia sintáctica predicha.\n",
    "                                                               # El atributo .head devuelve el token de la cabeza de la dependencia sintáctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con uno señora 5 de San Nicolás Que MISC\n",
      "Me PER\n",
      "Con esta 40 señorita me caso yo. MISC\n",
      "Arroz PER\n",
      "Con una señorita de san nicolás Que sepa  la cocer MISC\n",
      "Me PER\n",
      "Con esta señorita me caso yo MISC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Miscellaneous entities, e.g. events, nationalities, products or works of art'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entity recognizer (entidades nombradas: objetos de la vida real que tienen un nombre asignado)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la etiqueta de la entidad\n",
    "    print(ent.text, ent.label_) # La propiedad doc.ents te permite acceder a las entidades nombradas predichas por el modelo de reconocimiento de entidades.\n",
    "                                # La propiedad doc.ents devuelve un iterador de objetos Span, \n",
    "                                # así que podemos imprimir en pantalla el texto y la etiqueta de la entidad usando el atributo .label_.\n",
    "\n",
    "spacy.explain(\"MISC\") # Explica las definiciones de las etiquetas.\n",
    "# Mal resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepa el abrir\n"
     ]
    }
   ],
   "source": [
    "# Matcher. permite escribir reglas para encontrar palabras y frases en el texto.\n",
    "# Funciona con objetos Doc y Span, no solo con strings.\n",
    "# Identifica la función de una palabra teniendo en cuenta su contexto.\n",
    "# Los patrones del Matcher son listas de diccionarios. Cada diccionario describe un Token.\n",
    "\n",
    "# Busca por textos exactos de tokens\n",
    "#patternExacto = [{\"TEXT\": \"Arroz\"}, {\"TEXT\": \"con\"}] # Crea el patron\n",
    "#matcher.add(\"PATRON_TEXTO\",[patternExacto]) # Añadir el patron\n",
    "\n",
    "#Busca por atributos léxicos\n",
    "#patternMayus = [{\"LOWER\": \"san\"}, {\"LOWER\": \"nicolás\"}]\n",
    "#matcher.add(\"PATRON_MAYUS\",[patternMayus])               # NO SABEMOS PORQUE LO DEVUELVE 2 VECES (creemos que porque encuntra\n",
    "                                                         # uno en mayus y devuelve como si fuese uno en mayus y otro en minus)\n",
    "\n",
    "# Busca por cualquier atributo del token\n",
    "#patternRaiz = [{\"LEMMA\": \"querer\"}, {\"POS\": \"VERB\"}]\n",
    "#matcher.add(\"PATRON_RAIZ\",[patternRaiz])\n",
    "\n",
    "#patternDigito = [{\"IS_DIGIT\": True}]\n",
    "#matcher.add(\"PATRON_DIGITO\",[patternDigito])\n",
    "\n",
    "#patternSigno = [{\"IS_PUNCT\": True}]\n",
    "#matcher.add(\"PATRON_SIGNO\",[patternSigno])\n",
    "\n",
    "# Los operadores y cuantificadores te permiten definir con qué frecuencia un token debe ser encontrado. \n",
    "# Pueden ser añadidos con el key \"OP\".\n",
    "\n",
    "# \"OP\" puede tener uno de cuatro valores:\n",
    "# Un \"!\" niega el token, así que es buscado 0 veces.\n",
    "# Un \"?\" hace que el token sea opcional y es buscado 0 o 1 veces.\n",
    "# Un \"+\" busca el token 1 o más veces.\n",
    "# Finalmente, un \"*\" busca 0 o más veces.\n",
    "\n",
    "patternOpcional = [{\"TEXT\": \"sepa\"},{\"POS\": \"DET\", \"OP\": \"+\"},{\"POS\": \"VERB\"}]  # opcional: encuentra 0 o 1 ocurrencias\n",
    "matcher.add(\"PATRON_OPCIONAL\",[patternOpcional])\n",
    "\n",
    "\n",
    "matches = matcher(doc)   # Devuelve una lista de tuples.\n",
    "                         # Cada Tuple consiste de 3 valores: ID del resultado, indice de inicio y indice de final del span resultante.\n",
    "\n",
    "for match_id, start,end in matches: \n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 32833993555699147\n",
      "32833993555699147\n",
      "café\n"
     ]
    }
   ],
   "source": [
    "# spaCy guarda todos los datos en un vocabulario, el Vocab.\n",
    "# Este incluye palabras, pero también los esquemas de etiquetas y entidades.\n",
    "# Para usar menos memoria, todos los strings son codificados a hash IDs. \n",
    "# Si una palabra ocurre múltiples veces, no tenemos que guardarla cada vez.\n",
    "# En cambio, spaCy usa una función hash para generar un ID y guarda el string una vez en el string store. \n",
    "# El string store está disponible como nlp.vocab.strings\n",
    "\n",
    "# Es una lookup table que funciona en ambas direcciones. Puedes buscar un string y obtener su hash, \n",
    "# así como buscar un hash para obtener su valor string. Internamente spaCy solo se comunica en hash IDs.\n",
    "\n",
    "# Los hash IDs no se pueden revertir. Si una palabra no está en el vocabulario no hay forma de obtener su string. \n",
    "# Es por esto que siempre tenemos que pasar el vocabulario compartido.\n",
    "\n",
    "nlp.vocab.strings.add(\"café\") # Vocab: guarda los datos compartidos a través de múltiples documentos.\n",
    "cafe_hash = nlp.vocab.strings[\"café\"] # Guarda el hash del string \"café\".\n",
    "cafe_string = nlp.vocab.strings[cafe_hash] # Guarda el string \"café\" dado su hash.\n",
    "doc1 = nlp(\"Ines toma café\")\n",
    "print(\"hash value:\", doc1.vocab.strings[\"café\"])\n",
    "print(cafe_hash)\n",
    "print(cafe_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arroz 4826447396574439791 True\n"
     ]
    }
   ],
   "source": [
    "# Los lexemas son entradas en el vocabulario independientes del contexto.\n",
    "# Puedes obtener un lexema buscando un string o un hash ID en el vocabulario.\n",
    "# Los lexemas exponen atributos, al igual que los tokens.\n",
    "# Ellos contienen información sobre una palabra independiente del contexto, \n",
    "# como el texto o si la palabra está compuesta por caracteres alfanuméricos.\n",
    "# Los lexemas no tienen etiquetas gramaticales, dependencias o etiquetas de entidades, ya que esos dependen del contexto.\n",
    "\n",
    "lexeme = nlp.vocab[\"Arroz\"]\n",
    "\n",
    "# Imprime en pantalla los atributos léxicos\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontré un nombre propio antes de un verbo: leche\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Busco un sustantivo seguido de un verbo.\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":  # Revisa si el token actual es un nombre propio\n",
    "        if doc[token.i + 1].pos_ == \"VERB\": # Revisa si el siguiente token es un verbo\n",
    "            print(\"Encontré un nombre propio antes de un verbo:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMENTO TODO PORQUE PIERDO LAS RELACIONES DE LAS PALABRAS.\n",
    "\n",
    "\n",
    "# Crear un doc manualmente.\n",
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "#words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "#spaces = [False, True, False, False] # Los espacios son una lista de valores booleanos que indican si una palabra está seguida por un espacio.\n",
    "\n",
    "# Crea un doc manualmente\n",
    "#doc = Doc(nlp.vocab, words=words, spaces=spaces) # La clase Doc recibe tres argumentos: el vocabulario compartido, las palabras y los espacios.\n",
    "#print(doc)\n",
    "\n",
    "#span = Span(doc, 1, 3) # El Span recibe al menos tres argumentos: el doc al que se refiere, el índice de inicio y el índice del final del span.\n",
    "#print(span)\n",
    "\n",
    "# Crea un span con un label\n",
    "#span_with_label = Span(doc, 1, 3, label=\"SALUDO\")\n",
    "\n",
    "# Añade el span a los doc.ents\n",
    "#doc.ents = [span_with_label]\n",
    "\n",
    "# Si la aplicación tiene que generar strings asegúrate de convertir el doc lo más tarde posible. \n",
    "# Si lo haces demasiado temprano perderás las relaciones entre los tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8463697235674686\n",
      "[-0.33539    0.17083   -0.62339    2.017     -1.3763    -1.1457\n",
      " -0.72034    2.4666     2.508     -2.0996    -0.94729    0.31961\n",
      "  2.4047     2.5066     0.7299     1.1566    -1.9779     2.9473\n",
      "  0.74463    1.354      0.79521   -2.1274     1.7073    -1.0885\n",
      "  1.5979    -1.3678     1.1434     5.4932     3.2241    -0.97814\n",
      "  0.36803    0.80475    2.3822    -0.3006     0.97192    2.4423\n",
      "  1.5567     2.0621     2.3309    -1.4673     1.673     -0.99992\n",
      " -0.93373    1.0229    -2.2532     0.49394   -1.2381    -0.53449\n",
      " -1.8662     3.048     -1.1243    -3.2422    -1.2709    -0.8209\n",
      "  1.0236    -1.4516    -2.1556    -0.71552   -0.68864   -3.4261\n",
      " -0.38578    1.9612     0.57291   -1.9158     2.1337     0.29352\n",
      " -0.64589    0.94083   -0.63975    2.7674     0.72211    1.8376\n",
      "  1.0876     2.6115     0.36064   -2.4631     1.1471     1.8923\n",
      "  0.85355   -2.8397     0.99956    1.0787     0.29674    0.21393\n",
      "  0.013852   1.4735    -1.8616     3.2102    -0.18652   -1.6293\n",
      " -1.6858     0.091059  -0.31211    1.4973     2.8254     0.093434\n",
      " -0.33156   -0.22961   -2.1262    -0.51166   -1.8345     2.0881\n",
      "  1.8424     1.5618     2.2349     1.1048    -2.9844     3.2571\n",
      "  2.3087    -1.7786     0.82545    0.72307    2.72      -2.2496\n",
      "  2.8171    -2.2156    -0.85243    0.16955   -1.8974     0.8233\n",
      "  2.3057     0.93747    0.4199    -2.2823     1.3019    -2.6424\n",
      "  2.6403    -1.7394     0.89575   -1.8596    -3.0436    -0.84555\n",
      "  0.55743    0.6066     1.9739     1.9661     1.862      4.5233\n",
      " -2.2207    -0.49489    1.4152     1.8501     1.3244    -0.56448\n",
      " -0.3517     0.69606   -1.6369    -0.64527    1.9674    -4.0669\n",
      "  0.78631   -1.0986    -1.1972     1.641      3.0383     0.82893\n",
      "  0.81366   -1.5656     2.9972    -2.3419     5.965      0.24551\n",
      " -0.57665   -0.32472    3.0463    -3.3367    -1.1887    -1.7097\n",
      "  2.0552     1.6162    -2.1296    -2.321      0.96665   -2.2896\n",
      "  0.89544    0.55632    1.9784     1.1521    -1.3366    -0.54863\n",
      " -1.731      2.1911    -0.13635   -0.64919   -0.037768  -0.83801\n",
      " -1.0508    -2.1496    -0.25276    1.0372     1.8684    -2.7945\n",
      "  1.0313     4.7331    -2.4294    -0.96119    0.35726   -2.6111\n",
      " -5.2655     0.55938    0.24592   -0.53708   -2.7032    -1.4368\n",
      "  1.2263    -2.5538    -2.6802     0.48464    0.3423     0.82866\n",
      "  0.0087905  1.5865     1.8653     1.4617    -0.27897   -2.712\n",
      "  3.3655    -0.12486   -1.6015     0.45511   -0.82857   -1.3394\n",
      " -0.14475    6.3228     3.811     -0.74448    0.7125     0.22524\n",
      " -0.60075    0.62479    2.0898    -0.69066   -2.7149     2.3757\n",
      " -0.73766    0.070399   0.55959    0.92203    3.474      1.2301\n",
      " -1.7366     1.3309    -2.4544    -3.0099    -1.8468    -1.2104\n",
      " -0.21102    1.1613    -1.1699    -0.78957   -0.24477   -1.8999\n",
      " -1.6732     0.056276   2.3411     1.1161     2.2887     2.0585\n",
      " -1.9764     3.8335     1.2383     1.3033    -1.134     -0.64137\n",
      "  2.8835     3.7554    -1.7414     0.49617    2.0015    -1.7522\n",
      "  2.364     -0.70474    0.73317   -0.65593   -0.61051    2.3089\n",
      "  2.0952     0.12264    0.25985   -0.55278   -1.4369    -3.9589\n",
      " -0.22839   -0.45653    3.151      5.5085    -1.0098     0.65005\n",
      " -0.29008    0.92504   -3.5049     0.60357    1.0793    -0.78077\n",
      "  2.7825     1.1116     1.4941     2.2996     0.73757    1.6758   ]\n"
     ]
    }
   ],
   "source": [
    "# Comparar similitud entre un span con un documento\n",
    "span = nlp(\"Me gusta la pizza con queso\")[3:6]\n",
    "doc = nlp(\"McDonalds vende hamburguesas con queso\")\n",
    "\n",
    "print(span.similarity(doc))\n",
    "\n",
    "# La similitud se determina usando word vectors, que son representaciones multidimensionales de los significados de las palabras.\n",
    "\n",
    "# Accede al vector a través del atributo token.vector\n",
    "print(doc[1].vector) # vector de la palabra \"Arroz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
